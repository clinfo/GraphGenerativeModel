import logging
from lib.calculators import AbstractCalculator
from rdkit import Chem
import numpy as np
import pickle
from lib.helpers import Sketcher

class Evaluation(object):
    """
    Keep track of the molecule generated during the training as well as some other metrics.
    """
    def __init__(self, experiment_name: str, reward_calculator: AbstractCalculator) -> None:
        self.stats = []
        self.outputs = []
        self.experiment_name = experiment_name
        self.calculator = reward_calculator
    def add_output(self, output):
        # Retrieve multiple information of the molecules generated by the adj matrix
        # TODO Needs to reduce the amount of information store once the metric is clear
        score =[]
        bond_number = []
        mol = []
        smiles = []
        self.outputs.append(output)
        for o in output:
            # Case of old model
            if len(o["smiles"].split('.')) > 1:
                mols = [Chem.MolFromSmiles(s) for s in o["smiles"].split('.')]
                mol_bonds = [len(m.GetBonds()) for m in mols]
                id_biggest_mol = np.argmax(mol_bonds)
                bond_number.append(mol_bonds[id_biggest_mol])
                mol.append(mols[id_biggest_mol])
                smiles.append(o["smiles"].split('.')[id_biggest_mol])
                score.append(self.calculator.calculate(mols[id_biggest_mol]))
            else:
                bond_number.append(len(Chem.MolFromSmiles(o["smiles"]).GetBonds()))
                mol.append(Chem.MolFromSmiles(o["smiles"]))
                smiles.append(o["smiles"])
                score.append(o["score"])

        stat = {
            "bond_number": bond_number,
            "mol": mol,
            "smiles": smiles,
            "score": score,
            "max_reward": np.max(score),
            "mean_reward": np.mean(score),
            "min_reward": np.min(score),
            "max_bond": np.max(bond_number),
            "mean_bond": np.mean(bond_number),
            "min_bond": np.min(bond_number)
        }
        self.stats.append(stat)

    def save_stat(self, config):
        output = {
            'name': self.experiment_name,
            'config': config,
            'stat': self.stats,
            'output': self.outputs
        }
        with open(f'{self.experiment_name}.pkl', 'wb') as handle:
            pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)

        logging.info(f"Saved eval result to {self.experiment_name}.pkl")

def compare_evaluation(filename_a, filename_b):
    """
    Take two file generated with evaluation and compare them.
    Generate the molecule of each file.
    TODO Real metric comparison
    """
    def load(filename):
        with open(filename, 'rb') as handle:
            data = pickle.load(handle)
        return data
    output_a = load(filename_a)
    output_b = load(filename_b)
    assert len(output_a['stat']) == len(output_b['stat']), "The two evaluation don't have the same size"
    sketcher = Sketcher(output_a["name"])
    sketcher.draw_from_eval(output_a)
    sketcher = Sketcher(output_b["name"])
    sketcher.draw_from_eval(output_b)

if __name__ == "__main__":
    compare_evaluation("base_model.pkl", "mcts_test.pkl")